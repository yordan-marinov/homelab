apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  source:
    chart: rook-ceph-cluster
    repoURL: https://charts.rook.io/release
    targetRevision: 1.15.5
    helm:
      values: |
        operatorNamespace: rook-ceph
        cephClusterSpec:
          cephVersion:
            image: quay.io/ceph/ceph:v19.2.0

          # 1. Global Settings for 2-Node Cluster
          cephConfig:
            global:
              osd_pool_default_size: "2"
              osd_pool_default_min_size: "1"
              mon_allow_pool_delete: "true" # Allows GitOps to clean up deleted pools

          dataDirHostPath: /var/lib/rook

          mon:
            count: 3
            allowMultiplePerNode: false # Spread brains across all 3 workers

          mgr:
            count: 1
            dashboard:
              enabled: true
              ssl: true

          # 2. Block Storage (RBD) for Databases/Kafka
          cephBlockPools:
            - name: replicapool
              spec:
                failureDomain: host
                replicated:
                  size: 2
                  requireSafeReplica: false
              storageClass:
                enabled: true
                name: ceph-block
                isDefault: true
                reclaimPolicy: Delete
                allowVolumeExpansion: true

          # 3. Shared Filesystem (CephFS) for Media/Sync
          cephFilesystems:
            - name: ceph-filesystem
              spec:
                metadataPool:
                  replicated:
                    size: 2
                dataPools:
                  - name: data0
                    failureDomain: host
                    replicated:
                      size: 2
                metadataServer:
                  activeCount: 1
                  activeStandby: true
              storageClass:
                enabled: true
                name: ceph-filesystem
                reclaimPolicy: Delete
                allowVolumeExpansion: true

          # 4. Object Store (S3/RGW) - REFACTORED TO REPLICATED
          cephObjectStores:
            - name: ceph-objectstore
              spec:
                metadataPool:
                  replicated:
                    size: 2
                dataPool:
                  replicated:
                    size: 2 # No Erasure Coding until 3rd disk arrives
                gateway:
                  port: 80
                  instances: 1
              storageClass:
                enabled: true
                name: ceph-bucket
                reclaimPolicy: Delete

          crashCollector:
            disable: false

          # 5. Resource Limits for Dell Micros
          resources:
            mgr:
              limits: { memory: "512Mi" }
              requests: { cpu: "100m", memory: "256Mi" }
            mon:
              limits: { memory: "1024Mi" }
              requests: { cpu: "100m", memory: "512Mi" }
            osd:
              limits: { memory: "2Gi" }
              requests: { cpu: "200m", memory: "1Gi" }

          # 6. Physical Drive Mapping
          storage:
            useAllNodes: false
            useAllDevices: false
            config:
              osdsPerDevice: "1"
            nodes:
              - name: "k8s-w-1"
                devices: [{name: "nvme1n1"}] #2TB Crucial P3
              - name: "k8s-w-2"
                devices: [{name: "nvme0n1"}] #2TB Samsung 990
              - name: "k8s-w-3"
                devices: [] # Brain only, no disk

          priorityClassNames:
            all: system-node-critical

  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
    ignoreDifferences:
      - kind: Secret
        jsonPointers: [/data]
      - kind: ConfigMap
        jsonPointers: [/data]
      - group: ceph.rook.io
        kind: CephCluster
        jsonPointers: [/status]
